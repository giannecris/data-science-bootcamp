{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processamento de Linguagem Natural - Gabarito"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O objetivo do PLN é fornecer aos computadores a capacidade de entender e compor textos. “Entender” um texto significa reconhecer o contexto, fazer análise sintática, semântica, léxica e morfológica, criar resumos, extrair informação, interpretar os sentidos, analisar sentimentos e até aprender conceitos com os textos processados.\n",
    "\n",
    "\n",
    "Neste notebook, exploraremos dois problemas clássicos de PLN: `classificação de texto` e `agrupamento de tópicos`;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bibliotecas Auxiliares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/thaisalmeida/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from re import sub\n",
    "\n",
    "from numpy import asarray\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import PorterStemmer \n",
    "from nltk import download\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from warnings import filterwarnings\n",
    "\n",
    "filterwarnings('ignore')\n",
    "download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prática I : Identificação de Fake News"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://media.giphy.com/media/26n6ziTEeDDbowBkQ/giphy.gif\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carregando base de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_set = pd.read_csv('datasets/fakenews_silverman.csv')\n",
    "real_set = pd.read_csv('datasets/realnews_silverman.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|fake news| = 467 samples \n",
      "|legitimate news| = 467 samples\n"
     ]
    }
   ],
   "source": [
    "print(f'|fake news| = {fake_set.shape[0]} samples \\n|legitimate news| = {real_set.shape[0]} samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>main_content</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AUSTRALIA: 600-POUND WOMAN GIVES BIRTH TO 40-P...</td>\n",
       "      <td>Perth | A 600-pound woman has given birth to a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jonathan S. Geller</td>\n",
       "      <td>Apple has been hard at work on multiple upcomi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Amazon Is Opening a Brick-and-Mortar Store in ...</td>\n",
       "      <td>Amazon, the cyber store that sells everything,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            headline  \\\n",
       "0  AUSTRALIA: 600-POUND WOMAN GIVES BIRTH TO 40-P...   \n",
       "1                                 Jonathan S. Geller   \n",
       "2  Amazon Is Opening a Brick-and-Mortar Store in ...   \n",
       "\n",
       "                                        main_content  label  \n",
       "0  Perth | A 600-pound woman has given birth to a...      0  \n",
       "1  Apple has been hard at work on multiple upcomi...      0  \n",
       "2  Amazon, the cyber store that sells everything,...      0  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_set.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>main_content</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Apple’s next major Mac revealed: the radically...</td>\n",
       "      <td>Apple is preparing an all-new MacBook Air for ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Report: A Radically Redesigned 12-Inch MacBook...</td>\n",
       "      <td>Everyone's been waiting years and years for a ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Apple may launch 12-inch MacBook Air with Reti...</td>\n",
       "      <td>Apple would never lower itself to rubbing elbo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            headline  \\\n",
       "0  Apple’s next major Mac revealed: the radically...   \n",
       "1  Report: A Radically Redesigned 12-Inch MacBook...   \n",
       "2  Apple may launch 12-inch MacBook Air with Reti...   \n",
       "\n",
       "                                        main_content  label  \n",
       "0  Apple is preparing an all-new MacBook Air for ...      1  \n",
       "1  Everyone's been waiting years and years for a ...      1  \n",
       "2  Apple would never lower itself to rubbing elbo...      1  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real_set.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|corpus| = 934 samples\n"
     ]
    }
   ],
   "source": [
    "news_list = pd.concat([fake_set['headline'],real_set['headline']], axis=0, ignore_index=True)\n",
    "target_list = pd.concat([fake_set['label'],real_set['label']], axis=0, ignore_index=True)\n",
    "\n",
    "print(f'|corpus| = {news_list.shape[0]} samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limpeza de dados + Engenharia de Atributos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def remove_stopwords_and_normalize(doc_text, stopwords_hash):\n",
    "    content = []\n",
    "    stemmer = PorterStemmer() \n",
    "    \n",
    "    for word in doc_text:\n",
    "        word_clean = word.lower().strip()\n",
    "        if(stopwords_hash.get(word_clean) == None):\n",
    "            word_clean = stemmer.stem(word_clean)    \n",
    "            content.append(word_clean)\n",
    "    return content\n",
    "\n",
    "def tokenizer(text):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "def data_cleaning(news_list, target_list):\n",
    "    X_clean, Y_clean = [], []\n",
    "    \n",
    "    stopwords_dict = {word:0 for word in stopwords.words('english')}    \n",
    "    \n",
    "    for idx, news in enumerate(news_list):\n",
    "        text = sub(r'[^\\w\\s]',' ', news)\n",
    "        text = sub(r'[^\\D]',' ', text)\n",
    "        text = tokenizer(text)\n",
    "        text = remove_stopwords_and_normalize(text, stopwords_dict)\n",
    "        text = ' '.join(text).strip()\n",
    "        \n",
    "        if(len(text) > 0):\n",
    "            X_clean.append(text)\n",
    "            Y_clean.append(target_list[idx])\n",
    "    return X_clean, Y_clean "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = data_cleaning(news_list, target_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Engenharia de Atributos + Classificação de Texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fake       0.82      0.87      0.85        94\n",
      "  legitimate       0.86      0.81      0.84        94\n",
      "\n",
      "    accuracy                           0.84       188\n",
      "   macro avg       0.84      0.84      0.84       188\n",
      "weighted avg       0.84      0.84      0.84       188\n",
      " \n",
      "\n",
      "\n",
      "Fold: 2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fake       0.77      0.82      0.79        94\n",
      "  legitimate       0.81      0.76      0.78        94\n",
      "\n",
      "    accuracy                           0.79       188\n",
      "   macro avg       0.79      0.79      0.79       188\n",
      "weighted avg       0.79      0.79      0.79       188\n",
      " \n",
      "\n",
      "\n",
      "Fold: 3\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fake       0.79      0.86      0.82        93\n",
      "  legitimate       0.85      0.77      0.81        93\n",
      "\n",
      "    accuracy                           0.82       186\n",
      "   macro avg       0.82      0.82      0.82       186\n",
      "weighted avg       0.82      0.82      0.82       186\n",
      " \n",
      "\n",
      "\n",
      "Fold: 4\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fake       0.81      0.80      0.80        93\n",
      "  legitimate       0.80      0.82      0.81        93\n",
      "\n",
      "    accuracy                           0.81       186\n",
      "   macro avg       0.81      0.81      0.81       186\n",
      "weighted avg       0.81      0.81      0.81       186\n",
      " \n",
      "\n",
      "\n",
      "Fold: 5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fake       0.80      0.81      0.80        93\n",
      "  legitimate       0.80      0.80      0.80        93\n",
      "\n",
      "    accuracy                           0.80       186\n",
      "   macro avg       0.80      0.80      0.80       186\n",
      "weighted avg       0.80      0.80      0.80       186\n",
      " \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X, y = asarray(X), asarray(y)\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=5, random_state=42, shuffle=True)\n",
    "\n",
    "iteration = 1\n",
    "for train_index, test_index in kfold.split(X, y):\n",
    "\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    Y_train, Y_test = y[train_index], y[test_index]\n",
    "\n",
    "    vectorizer = TfidfVectorizer(use_idf=True, ngram_range = (1,1),\\\n",
    "                     min_df = 5, max_df = 0.70)\n",
    "\n",
    "    X_train = vectorizer.fit_transform(X_train)\n",
    "    X_test  = vectorizer.transform(X_test)\n",
    "\n",
    "    classifier = RandomForestClassifier(random_state=5)\n",
    "    classifier.fit(X_train, Y_train)\n",
    "    predictions = classifier.predict(X_test)\n",
    "    \n",
    "    print(f'Fold: {iteration}')\n",
    "    print(classification_report(Y_test, predictions, target_names=['fake','legitimate']),'\\n\\n')\n",
    "    iteration+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Desafio:\n",
    "\n",
    "- Criar um modelo de identificação de notícias falsas utilizando o `conteúdo` das notícias representado por `bigramas` ponderados por TF-IDF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img height=\"50\" width=\"300\" src=\"https://media.giphy.com/media/l2YWs1NexTst9YmFG/giphy.gif\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prática II : Agrupamento em Tópicos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carregando base de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_set = pd.read_csv('datasets/fakenews_silverman.csv')\n",
    "real_set = pd.read_csv('datasets/realnews_silverman.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|fake news| = 467 samples \n",
      "|legitimate news| = 467 samples\n"
     ]
    }
   ],
   "source": [
    "print(f'|fake news| = {fake_set.shape[0]} samples \\n|legitimate news| = {real_set.shape[0]} samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|corpus| = 934 samples\n"
     ]
    }
   ],
   "source": [
    "news_list = pd.concat([fake_set['headline'],real_set['headline']], axis=0, ignore_index=True)\n",
    "target_list = pd.concat([fake_set['label'],real_set['label']], axis=0, ignore_index=True)\n",
    "\n",
    "print(f'|corpus| = {news_list.shape[0]} samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_cluster_terms(km, tfidf_vectorizer, number_of_clusters):\n",
    "    order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
    "    terms = tfidf_vectorizer.get_feature_names()\n",
    "    dist = clusters_distribution(km)\n",
    "    \n",
    "    top_ten_list, dist_list = [],[]\n",
    "    for i in range(number_of_clusters):\n",
    "        top_ten_words = [terms[ind] for ind in order_centroids[i, :7]]\n",
    "        print(\"Cluster \",i,f'| Total: {dist[i]}|',' '.join(top_ten_words),)\n",
    "        \n",
    "def clusters_distribution(km):\n",
    "    clusters_count = {}\n",
    "    for i in km.labels_:\n",
    "        if(clusters_count.get(i)!=None):\n",
    "            clusters_count[i]+=1\n",
    "        else:\n",
    "            clusters_count[i]=1\n",
    "    return clusters_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(doc_text, stopwords_hash):\n",
    "    content = []\n",
    "    \n",
    "    for word in doc_text:\n",
    "        word_clean = word.lower().strip()\n",
    "        if(stopwords_hash.get(word_clean) == None):\n",
    "            content.append(word_clean)\n",
    "    return content\n",
    "\n",
    "def tokenizer(text):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "def data_cleaning(news_list):\n",
    "    X_clean = []\n",
    "    \n",
    "    stopwords_dict = {word:0 for word in stopwords.words('english')}    \n",
    "    \n",
    "    for idx, news in enumerate(news_list):\n",
    "        text = sub(r'[^\\w\\s]',' ', news)\n",
    "        text = sub(r'[^\\D]',' ', text)\n",
    "        text = tokenizer(text)\n",
    "        text = remove_stopwords(text, stopwords_dict)\n",
    "        text = ' '.join(text).strip()\n",
    "        \n",
    "        if(len(text) > 0):\n",
    "            X_clean.append(text)\n",
    "    return X_clean "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_clean = data_cleaning(news_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster  0 | Total: 179| banksy arrested argentina president batmobile stolen stolen detroit turning werewolf identity revealed president adopts\n",
      "Cluster  1 | Total: 508| boko haram third breast bank hank big bank sugarhill gang vladimir putin jose canseco\n",
      "Cluster  2 | Total: 24| justin bieber bear attack bieber ringtone russian fisherman saves man saves russian mauled bear\n",
      "Cluster  3 | Total: 26| rescue attempt luke somers attempt yemen yemen rescue british born rescue bid killed rescue\n",
      "Cluster  4 | Total: 76| apple watch macbook air inch macbook watch edition gold apple steel apple stainless steel\n",
      "Cluster  5 | Total: 47| islamic state james foley killed us us journalist missing american journalist james american journalist\n",
      "Cluster  6 | Total: 26| james wright wright foley journalist james american journalist beheads american isis beheads islamic state\n",
      "Cluster  7 | Total: 24| isis fighters contracted ebola fighters contracted isis militants iraqi media media reports reports isis\n",
      "Cluster  8 | Total: 15| hewlett packard two companies split two plans break packard plans break companies packard split\n",
      "Cluster  9 | Total: 9| brian williams brokaw wants wants brian tom brokaw williams fired meteorologist peeing williams meteorologist\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(use_idf=True, sublinear_tf=False, ngram_range=(2,2))\n",
    "X = vectorizer.fit_transform(X_clean)\n",
    "\n",
    "kmeans = KMeans(n_clusters=10, random_state = 42).fit(X)\n",
    "\n",
    "top_cluster_terms(kmeans, vectorizer, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster  0 | Total: 390| banksy hoax woman isis saudi caught arrested\n",
      "Cluster  1 | Total: 29| haram boko nigeria ceasefire girls kidnapped schoolgirls\n",
      "Cluster  2 | Total: 153| batmobile president werewolf argentina stolen boy million\n",
      "Cluster  3 | Total: 42| killed rescue yemen attempt al hostage us\n",
      "Cluster  4 | Total: 90| apple watch gold macbook air inch cost\n",
      "Cluster  5 | Total: 66| claims isis islamic state weapons airdrop us\n",
      "Cluster  6 | Total: 76| ebola bear bieber justin contracted isis man\n",
      "Cluster  7 | Total: 46| journalist james foley american wright video beheaded\n",
      "Cluster  8 | Total: 23| companies two packard hewlett split hp break\n",
      "Cluster  9 | Total: 19| bank gang hank sugarhill big canadian captured\n"
     ]
    }
   ],
   "source": [
    "X_clean = data_cleaning(news_list)\n",
    "\n",
    "vectorizer = TfidfVectorizer(use_idf=True, sublinear_tf=False)\n",
    "X = vectorizer.fit_transform(X_clean)\n",
    "\n",
    "kmeans = KMeans(n_clusters=10, random_state = 42).fit(X)\n",
    "\n",
    "top_cluster_terms(kmeans, vectorizer, 10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Referências:\n",
    "\n",
    "- https://www.amazon.com.br/Express%C3%B5es-Regulares-Uma-Abordagem-Divertida/dp/8575223372\n",
    "- Baeza-Yates, Ricardo, and Berthier Ribeiro-Neto. Recuperação de Informação-: Conceitos e Tecnologia das Máquinas de Busca. Bookman Editora, 2013.\n",
    "- https://medium.com/botsbrasil/o-que-%C3%A9-o-processamento-de-linguagem-natural-49ece9371cff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
