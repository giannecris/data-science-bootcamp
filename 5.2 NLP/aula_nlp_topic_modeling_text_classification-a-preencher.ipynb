{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processamento de Linguagem Natural"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O objetivo do PLN é fornecer aos computadores a capacidade de entender e compor textos. “Entender” um texto significa reconhecer o contexto, fazer análise sintática, semântica, léxica e morfológica, criar resumos, extrair informação, interpretar os sentidos, analisar sentimentos e até aprender conceitos com os textos processados.\n",
    "\n",
    "\n",
    "Neste notebook, exploraremos dois problemas clássicos de PLN: `classificação de texto` e `agrupamento de tópicos`;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bibliotecas Auxiliares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/thaisalmeida/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from re import sub\n",
    "\n",
    "from numpy import asarray\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import PorterStemmer \n",
    "from nltk import download\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from warnings import filterwarnings\n",
    "\n",
    "filterwarnings('ignore')\n",
    "download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prática I : Identificação de Fake News"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://media.giphy.com/media/26n6ziTEeDDbowBkQ/giphy.gif\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carregando base de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_set = pd.read_csv('datasets/fakenews_silverman.csv')\n",
    "real_set = pd.read_csv('datasets/realnews_silverman.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|fake news| = 467 samples \n",
      "|legitimate news| = 467 samples\n"
     ]
    }
   ],
   "source": [
    "print(f'|fake news| = {fake_set.shape[0]} samples \\n|legitimate news| = {real_set.shape[0]} samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>main_content</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AUSTRALIA: 600-POUND WOMAN GIVES BIRTH TO 40-P...</td>\n",
       "      <td>Perth | A 600-pound woman has given birth to a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jonathan S. Geller</td>\n",
       "      <td>Apple has been hard at work on multiple upcomi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Amazon Is Opening a Brick-and-Mortar Store in ...</td>\n",
       "      <td>Amazon, the cyber store that sells everything,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            headline  \\\n",
       "0  AUSTRALIA: 600-POUND WOMAN GIVES BIRTH TO 40-P...   \n",
       "1                                 Jonathan S. Geller   \n",
       "2  Amazon Is Opening a Brick-and-Mortar Store in ...   \n",
       "\n",
       "                                        main_content  label  \n",
       "0  Perth | A 600-pound woman has given birth to a...      0  \n",
       "1  Apple has been hard at work on multiple upcomi...      0  \n",
       "2  Amazon, the cyber store that sells everything,...      0  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_set.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>main_content</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Apple’s next major Mac revealed: the radically...</td>\n",
       "      <td>Apple is preparing an all-new MacBook Air for ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Report: A Radically Redesigned 12-Inch MacBook...</td>\n",
       "      <td>Everyone's been waiting years and years for a ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Apple may launch 12-inch MacBook Air with Reti...</td>\n",
       "      <td>Apple would never lower itself to rubbing elbo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            headline  \\\n",
       "0  Apple’s next major Mac revealed: the radically...   \n",
       "1  Report: A Radically Redesigned 12-Inch MacBook...   \n",
       "2  Apple may launch 12-inch MacBook Air with Reti...   \n",
       "\n",
       "                                        main_content  label  \n",
       "0  Apple is preparing an all-new MacBook Air for ...      1  \n",
       "1  Everyone's been waiting years and years for a ...      1  \n",
       "2  Apple would never lower itself to rubbing elbo...      1  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real_set.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|corpus| = 934 samples\n"
     ]
    }
   ],
   "source": [
    "news_list = pd.concat([fake_set['headline'],real_set['headline']], axis=0, ignore_index=True)\n",
    "target_list = pd.concat([fake_set['label'],real_set['label']], axis=0, ignore_index=True)\n",
    "\n",
    "print(f'|corpus| = {news_list.shape[0]} samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Limpeza de dados + Engenharia de Atributos..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Engenharia de Atributos + Classificação de Texto...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Desafio:\n",
    "\n",
    "- Criar um modelo de identificação de notícias falsas utilizando o `conteúdo` das notícias representado por `bigramas` ponderados por TF-IDF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img height=\"50\" width=\"300\" src=\"https://media.giphy.com/media/l2YWs1NexTst9YmFG/giphy.gif\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prática II : Agrupamento em Tópicos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carregando base de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_set = pd.read_csv('datasets/fakenews_silverman.csv')\n",
    "real_set = pd.read_csv('datasets/realnews_silverman.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|fake news| = 467 samples \n",
      "|legitimate news| = 467 samples\n"
     ]
    }
   ],
   "source": [
    "print(f'|fake news| = {fake_set.shape[0]} samples \\n|legitimate news| = {real_set.shape[0]} samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|corpus| = 934 samples\n"
     ]
    }
   ],
   "source": [
    "news_list = pd.concat([fake_set['headline'],real_set['headline']], axis=0, ignore_index=True)\n",
    "target_list = pd.concat([fake_set['label'],real_set['label']], axis=0, ignore_index=True)\n",
    "\n",
    "print(f'|corpus| = {news_list.shape[0]} samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limpeza de dados + Engenharia de Atributos..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualização...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agrupamento de tópicos..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Referências:\n",
    "\n",
    "- https://www.amazon.com.br/Express%C3%B5es-Regulares-Uma-Abordagem-Divertida/dp/8575223372\n",
    "- Baeza-Yates, Ricardo, and Berthier Ribeiro-Neto. Recuperação de Informação-: Conceitos e Tecnologia das Máquinas de Busca. Bookman Editora, 2013.\n",
    "- https://medium.com/botsbrasil/o-que-%C3%A9-o-processamento-de-linguagem-natural-49ece9371cff"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
